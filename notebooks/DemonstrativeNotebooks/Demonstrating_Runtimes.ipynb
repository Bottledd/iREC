{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17eee984",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1166f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f1a79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kristophermiltiadou/Documents/UniWork/Cambridge/Thesis/CODE/iREC\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c227291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as dist\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from rec.utils import kl_estimate_with_mc, plot_running_sum_2d, plot_2d_distribution, kl_estimate_with_mc, compute_variational_posterior\n",
    "from tqdm.notebook import trange\n",
    "#import seaborn as sns; sns.set(); sns.set_style('whitegrid')\n",
    "import math\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706fe4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.weight': 'normal'})\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams.update({'lines.linewidth' : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dcd752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baff666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rec.beamsearch.Coders.Encoder_Variational import Encoder\n",
    "from rec.beamsearch.distributions.CodingSampler import CodingSampler\n",
    "from rec.beamsearch.distributions.VariationalPosterior import VariationalPosterior\n",
    "from rec.beamsearch.samplers.GreedySampling import GreedySampler\n",
    "from rec.OptimisingVars.VariationalOptimiser import VariationalOptimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f30f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "\n",
    "qs = []\n",
    "ps = []\n",
    "dims = [1, 5, 10, 25, 50]\n",
    "num_seeds = 500\n",
    "num_warmup_seeds = 150\n",
    "random_seeds = torch.randint(low=0, high=10000, size=(num_seeds+num_warmup_seeds,))\n",
    "for d in dims:\n",
    "    rand_mat = torch.rand(size=(d,d))\n",
    "    qs.append(dist.MultivariateNormal(loc= torch.ones(size=(d,)), covariance_matrix = torch.eye(d)))\n",
    "    ps.append(dist.MultivariateNormal(loc=torch.zeros((d,)), covariance_matrix = torch.eye(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28fd2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rec.beamsearch.Coders.Encoder_Variational import Encoder\n",
    "from rec.beamsearch.distributions.CodingSampler import CodingSampler\n",
    "from rec.beamsearch.distributions.VariationalPosterior import VariationalPosterior\n",
    "from rec.beamsearch.samplers.GreedySampling import GreedySampler\n",
    "from rec.OptimisingVars.VariationalOptimiser import VariationalOptimiser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bccf9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_sampler = CodingSampler\n",
    "auxiliary_posterior = VariationalPosterior\n",
    "selection_sampler = GreedySampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "559da845",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = 5\n",
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.\n",
    "var_performance = torch.zeros((len(dims), num_seeds, 1))\n",
    "var_times = torch.zeros((len(dims), num_seeds, 1))\n",
    "\n",
    "for i, q in enumerate(qs):\n",
    "    for j, seed in enumerate(random_seeds):\n",
    "        start = time.time()\n",
    "        enc = Encoder(q,\n",
    "                      seed,\n",
    "                     coding_sampler,\n",
    "                     selection_sampler,\n",
    "                     auxiliary_posterior,\n",
    "                     omega,\n",
    "                     beamwidth=beamwidth,\n",
    "                     epsilon=epsilon,\n",
    "                     prior_var=1.)\n",
    "        z, _ = enc.run_encoder()\n",
    "        perf = q.log_prob(z)\n",
    "        end = time.time()\n",
    "        run_time = end - start\n",
    "        \n",
    "        # lag at the start so wait 100 seeds before logging\n",
    "        if j > num_warmup_seeds:\n",
    "        # append stuff\n",
    "            var_performance[i, j - num_warmup_seeds] = perf.item()\n",
    "            var_times[i, j - num_warmup_seeds] = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a3d824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rec.beamsearch.Coders.Encoder import Encoder\n",
    "from rec.beamsearch.distributions.CodingSampler import CodingSampler\n",
    "from rec.beamsearch.distributions.EmpiricalMixturePosterior import EmpiricalMixturePosterior\n",
    "from rec.beamsearch.samplers.GreedySampling import GreedySampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e6ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_sampler = CodingSampler\n",
    "auxiliary_posterior = EmpiricalMixturePosterior\n",
    "selection_sampler = GreedySampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1ccd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = 5\n",
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.\n",
    "emp_1_performance = torch.zeros((len(dims), num_seeds, 1))\n",
    "emp_1_times = torch.zeros((len(dims), num_seeds, 1))\n",
    "\n",
    "for i, q in enumerate(qs):\n",
    "    for j, seed in enumerate(random_seeds):\n",
    "        start = time.time()\n",
    "        enc = Encoder(q,\n",
    "                      seed,\n",
    "                     coding_sampler,\n",
    "                     selection_sampler,\n",
    "                     auxiliary_posterior,\n",
    "                     omega,\n",
    "                      n_samples_from_target=1,\n",
    "                     beamwidth=beamwidth,\n",
    "                     epsilon=epsilon,\n",
    "                     prior_var=1.)\n",
    "        z, _ = enc.run_encoder()\n",
    "        perf = q.log_prob(z)\n",
    "        end = time.time()\n",
    "        run_time = end - start\n",
    "        \n",
    "        # lag at the start so wait 100 seeds before logging\n",
    "        if j > num_warmup_seeds:\n",
    "        # append stuff\n",
    "            emp_1_performance[i, j - num_warmup_seeds] = perf.item()\n",
    "            emp_1_times[i, j - num_warmup_seeds] = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6d9d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = 5\n",
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.\n",
    "emp_10_performance = torch.zeros((len(dims), num_seeds, 1))\n",
    "emp_10_times = torch.zeros((len(dims), num_seeds, 1))\n",
    "\n",
    "for i, q in enumerate(qs):\n",
    "    for j, seed in enumerate(random_seeds):\n",
    "        start = time.time()\n",
    "        enc = Encoder(q,\n",
    "                      seed,\n",
    "                     coding_sampler,\n",
    "                     selection_sampler,\n",
    "                     auxiliary_posterior,\n",
    "                     omega,\n",
    "                      n_samples_from_target=10,\n",
    "                     beamwidth=beamwidth,\n",
    "                     epsilon=epsilon,\n",
    "                     prior_var=1.)\n",
    "        z, _ = enc.run_encoder()\n",
    "        perf = q.log_prob(z)\n",
    "        end = time.time()\n",
    "        run_time = end - start\n",
    "        \n",
    "        # lag at the start so wait 100 seeds before logging\n",
    "        if j > num_warmup_seeds:\n",
    "        # append stuff\n",
    "            emp_10_performance[i, j - num_warmup_seeds] = perf.item()\n",
    "            emp_10_times[i, j - num_warmup_seeds] = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb473a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = 5\n",
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.\n",
    "emp_100_performance = torch.zeros((len(dims), num_seeds, 1))\n",
    "emp_100_times = torch.zeros((len(dims), num_seeds, 1))\n",
    "\n",
    "for i, q in enumerate(qs):\n",
    "    for j, seed in enumerate(random_seeds):\n",
    "        start = time.time()\n",
    "        enc = Encoder(q,\n",
    "                      seed,\n",
    "                     coding_sampler,\n",
    "                     selection_sampler,\n",
    "                     auxiliary_posterior,\n",
    "                     omega,\n",
    "                      n_samples_from_target=100,\n",
    "                     beamwidth=beamwidth,\n",
    "                     epsilon=epsilon,\n",
    "                     prior_var=1.)\n",
    "        z, _ = enc.run_encoder()\n",
    "        perf = q.log_prob(z)\n",
    "        end = time.time()\n",
    "        run_time = end - start\n",
    "        \n",
    "        # lag at the start so wait 100 seeds before logging\n",
    "        if j > num_warmup_seeds:\n",
    "        # append stuff\n",
    "            emp_100_performance[i, j - num_warmup_seeds] = perf.item()\n",
    "            emp_100_times[i, j - num_warmup_seeds] = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8362363",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = 5\n",
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.\n",
    "emp_1000_performance = torch.zeros((len(dims), num_seeds, 1))\n",
    "emp_1000_times = torch.zeros((len(dims), num_seeds, 1))\n",
    "\n",
    "for i, q in enumerate(qs):\n",
    "    for j, seed in enumerate(random_seeds):\n",
    "        start = time.time()\n",
    "        enc = Encoder(q,\n",
    "                      seed,\n",
    "                     coding_sampler,\n",
    "                     selection_sampler,\n",
    "                     auxiliary_posterior,\n",
    "                     omega,\n",
    "                      n_samples_from_target=1000,\n",
    "                     beamwidth=beamwidth,\n",
    "                     epsilon=epsilon,\n",
    "                     prior_var=1.)\n",
    "        z, _ = enc.run_encoder()\n",
    "        perf = q.log_prob(z)\n",
    "        end = time.time()\n",
    "        run_time = end - start\n",
    "        \n",
    "        # lag at the start so wait 100 seeds before logging\n",
    "        if j > num_warmup_seeds:\n",
    "        # append stuff\n",
    "            emp_1000_performance[i, j - num_warmup_seeds] = perf.item()\n",
    "            emp_1000_times[i, j - num_warmup_seeds] = run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q, p in zip(qs, ps):\n",
    "    print(dist.kl_divergence(q, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_mean_times = [emp_1_times.mean(1), emp_10_times.mean(1), emp_100_times.mean(1), emp_1000_times.mean(1)]\n",
    "emp_std_times = [emp_1_times.std(1), emp_10_times.std(1), emp_100_times.std(1), emp_1000_times.std(1)]\n",
    "var_mean_times = var_times.mean(1)\n",
    "var_std_times = var_times.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72169eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_mean_performance = [emp_1_performance.mean(1), emp_10_performance.mean(1), emp_100_performance.mean(1), emp_1000_performance.mean(1)]\n",
    "emp_std_performance = [emp_1_performance.std(1), emp_10_performance.std(1), emp_100_performance.std(1), emp_1000_performance.std(1)]\n",
    "var_mean_performance = var_performance.mean(1)\n",
    "var_std_performance = var_performance.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 7.5))\n",
    "fs = 16\n",
    "for times, stds in zip(emp_mean_times, emp_std_times):\n",
    "    ax.plot(dims, times, 'o-')\n",
    "    ax.fill_between(dims, (times + stds * 2).flatten(), (times - stds * 2).flatten(), alpha=0.25)\n",
    "    ax.set_yscale('log')\n",
    "ax.plot(dims, var_mean_times, 'o--', color='lime')\n",
    "ax.fill_between(dims, (var_mean_times + var_std_times * 2).flatten(), (var_mean_times - var_std_times * 2).flatten(), alpha=0.25, color='lime')\n",
    "ax.set_xlabel('Dimension', fontsize=fs)\n",
    "ax.set_ylabel('Runtime (s)', fontsize=fs)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fs-2)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=fs-2)\n",
    "ax.legend(['MOD-Scheme: 1 Sample', 'MOD-Scheme: 10 Samples', 'MOD-Scheme: 100 Samples', 'MOD-Scheme: 1000 Samples', 'FG-Scheme'], fontsize=fs)\n",
    "f.tight_layout()\n",
    "f.savefig(\"Figures/Thesis/Var_vs_Emp_runtimes.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15d483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
