{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10fede91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d37e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24022a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/km817/iREC\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "434ee3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hamiltorch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.distributions as D\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb00df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64a02a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamiltorch.set_random_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99d2284e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0.dev1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamiltorch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f8f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BNNs.DeterministicNN import Deterministic_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8eb7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds(preds, title='', fs=16, ax=None):\n",
    "    # plot the fit\n",
    "    fs = 16\n",
    "\n",
    "    m = preds.mean(0).to('cpu')\n",
    "    s = preds.std(0).to('cpu')\n",
    "    s_al = (preds.var(0).to('cpu') + tau_out ** -1) ** 0.5\n",
    "    \n",
    "    if ax==None:\n",
    "        f, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = (m - s*2).flatten(), (m + s*2).flatten()\n",
    "    # + aleotoric\n",
    "    lower_al, upper_al = (m - s_al*2).flatten(), (m + s_al*2).flatten()\n",
    "\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(x_data.numpy(), y_data.numpy(), 'k*', rasterized=True)\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(xs.numpy(), m.numpy(), 'b', rasterized=True)\n",
    "    ax.plot(xs.numpy(), ys.numpy(), 'r', rasterized=True)# Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(xs.flatten().numpy(), lower.numpy(), upper.numpy(), alpha=0.5, rasterized=True)\n",
    "    ax.fill_between(xs.flatten().numpy(), lower_al.numpy(), upper_al.numpy(), alpha=0.2, rasterized=True)\n",
    "    ax.grid()\n",
    "    ax.legend(['Observed Data', 'Mean', 'Ground Truth', 'Epistemic', 'Aleatoric'], fontsize = fs)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=14)\n",
    "    ax.set_title(title, fontsize=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff2b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_preds(preds_1, preds_2, title_1='', title_2='', fs=12):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # plot 1\n",
    "    m_1 = preds_1.mean(0).to('cpu')\n",
    "    s_1 = preds_1.std(0).to('cpu')\n",
    "    s_al_1 = (preds_1.var(0).to('cpu') + tau_out ** -1) ** 0.5\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower_1, upper_1 = (m_1 - s_1*2).flatten(), (m_1 + s_1*2).flatten()\n",
    "    # + aleotoric\n",
    "    lower_al_1, upper_al_1 = (m_1 - s_al_1*2).flatten(), (m_1 + s_al_1*2).flatten()\n",
    "\n",
    "    # Plot training data as black stars\n",
    "    ax1.plot(x_data.numpy(), y_data.numpy(), 'k*', rasterized=True)\n",
    "    # Plot predictive means as blue line\n",
    "    ax1.plot(xs.numpy(), m_1.numpy(), 'b', rasterized=True)\n",
    "    ax1.plot(xs.numpy(), ys.numpy(), 'r', rasterized=True)# Shade between the lower and upper confidence bounds\n",
    "    ax1.fill_between(xs.flatten().numpy(), lower_1.numpy(), upper_1.numpy(), alpha=0.5, rasterized=True)\n",
    "    ax1.fill_between(xs.flatten().numpy(), lower_al_1.numpy(), upper_al_1.numpy(), alpha=0.2, rasterized=True)\n",
    "    ax1.grid()\n",
    "    ax1.legend(['Observed Data', 'Mean', 'Ground Truth', 'Epistemic', 'Aleatoric'], fontsize = fs)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax1.tick_params(axis='both', which='minor', labelsize=14)\n",
    "    ax1.set_title(title_1, fontsize=fs)\n",
    "    \n",
    "    # plot 2\n",
    "    m_2 = preds_2.mean(0).to('cpu')\n",
    "    s_2 = preds_2.std(0).to('cpu')\n",
    "    s_al_2 = (preds_2.var(0).to('cpu') + tau_out ** -1) ** 0.5\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower_2, upper_2 = (m_2 - s_2*2).flatten(), (m_2 + s_2*2).flatten()\n",
    "    # + aleotoric\n",
    "    lower_al_2, upper_al_2 = (m_2 - s_al_2*2).flatten(), (m_2 + s_al_2*2).flatten()\n",
    "\n",
    "    # Plot training data as black stars\n",
    "    ax2.plot(x_data.numpy(), y_data.numpy(), 'k*', rasterized=True)\n",
    "    # Plot predictive means as blue line\n",
    "    ax2.plot(xs.numpy(), m_2.numpy(), 'b', rasterized=True)\n",
    "    ax2.plot(xs.numpy(), ys.numpy(), 'r', rasterized=True)# Shade between the lower and upper confidence bounds\n",
    "    ax2.fill_between(xs.flatten().numpy(), lower_2.numpy(), upper_2.numpy(), alpha=0.5, rasterized=True)\n",
    "    ax2.fill_between(xs.flatten().numpy(), lower_al_2.numpy(), upper_al_2.numpy(), alpha=0.2, rasterized=True)\n",
    "    ax2.grid()\n",
    "    ax2.legend(['Observed Data', 'Mean', 'Ground Truth', 'Epistemic', 'Aleatoric'], fontsize = fs)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax2.tick_params(axis='both', which='minor', labelsize=14)\n",
    "    ax2.set_title(title_2, fontsize=fs)\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d557495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create toy dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "torch.manual_seed(20)\n",
    "x = torch.cat([torch.Tensor(75).uniform_(-5, -2).sort()[0].reshape(-1, 1),\n",
    "               torch.Tensor(50).uniform_(2, 5).sort()[0].reshape(-1, 1)])\n",
    "i = 30\n",
    "x_data = torch.cat([x[0:i - 15], x[i + 14:]])\n",
    "\n",
    "# generate some data\n",
    "alpha, beta, num_nodes = 1., 100., 2\n",
    "\n",
    "# generate some data\n",
    "data_generator_model = Deterministic_NN(alpha=alpha, beta=beta, num_nodes=num_nodes)\n",
    "sampled_weights = data_generator_model.sample_weights_from_prior()\n",
    "data_generator_model.make_weights_from_sample(sampled_weights)\n",
    "y_data = data_generator_model(x_data).detach() + (\n",
    "            1 / data_generator_model.likelihood_beta ** 0.5) * torch.randn_like(\n",
    "    data_generator_model(x_data).detach())\n",
    "\n",
    "x_test = torch.Tensor(200).uniform_(-10., 10.).sort()[0]\n",
    "y_test = data_generator_model(x_test).detach() + (\n",
    "            1 / data_generator_model.likelihood_beta ** 0.5) * torch.randn_like(\n",
    "    data_generator_model(x_test).detach()).sort()[0]\n",
    "\n",
    "xs = torch.linspace(-10, 10, 100)\n",
    "ys = data_generator_model(xs).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=16\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(x_data,y_data, 'k*')\n",
    "plt.plot(xs, ys, 'r')\n",
    "plt.legend(['Observed Data', 'Ground Truth'], fontsize = fs)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8558460",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(x_data,y_data, 'k*')\n",
    "plt.plot(x_test,y_test, 'k+')\n",
    "plt.plot(xs, ys, 'r')\n",
    "plt.legend(['Observed Data', 'Ground Truth', 'Test Data'], fontsize = 16)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# if torch.cuda.is_available():\n",
    "#     print(torch.cuda.get_device_name(0))\n",
    "\n",
    "device  =torch.device('cpu')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_nodes: int = 10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, num_nodes)\n",
    "        self.fc2 = nn.Linear(num_nodes, num_nodes)\n",
    "        self.fc3 = nn.Linear(num_nodes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x.view(-1, 1)))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(num_nodes=num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a64b5",
   "metadata": {},
   "source": [
    "# MF-VI Approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaef14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BNNs.SimpleBBPBNN import SimpleBBPBNN, train_bnn\n",
    "from models.BNNs.pyroBNN_local_reparam import BayesianNeuralNetwork\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "import pyro\n",
    "import pyro.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models\n",
    "ELBO_BETA = .1\n",
    "model = BayesianNeuralNetwork(prior_var=1./alpha, likelihood_var=1./beta * ELBO_BETA, hidden_nodes=num_nodes)\n",
    "guide = AutoDiagonalNormal(model)\n",
    "\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO\n",
    "\n",
    "adam = pyro.optim.Adam({\"lr\": 0.1})\n",
    "svi = SVI(model, guide, adam, loss=TraceMeanField_ELBO())\n",
    "\n",
    "num_iterations = 10000\n",
    "pyro.clear_param_store()\n",
    "for j in range(num_iterations):\n",
    "    # calculate the loss and take a gradient step\n",
    "    loss = svi.step(x_data, y_data)\n",
    "    if j % 1000 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(x_data)))\n",
    "\n",
    "guide.requires_grad_(False)\n",
    "\n",
    "params = []\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    params.append(pyro.param(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3557d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = 'regression'\n",
    "\n",
    "# Effect of tau\n",
    "tau = alpha # Prior Precision\n",
    "tau_out = beta # Output Precision\n",
    "\n",
    "tau_list = []\n",
    "for w in net.parameters():\n",
    "    tau_list.append(tau) # set the prior precision to be the same for each set of weights\n",
    "tau_list = torch.tensor(tau_list).to(device)\n",
    "\n",
    "mean, stds = params\n",
    "\n",
    "variational_posterior = dist.MultivariateNormal(loc=mean, covariance_matrix=torch.diag(stds ** 2))\n",
    "\n",
    "variational_samples = variational_posterior.sample((1000,))\n",
    "pred_list_var_exact, log_probs_f = hamiltorch.predict_model(net, x = xs.to(device),\n",
    "                                                  y = xs.to(device), samples=variational_samples,\n",
    "                                                  model_loss=model_loss, tau_out=tau_out,\n",
    "                                                  tau_list=tau_list)\n",
    "plot_preds(pred_list_var_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fdfea1",
   "metadata": {},
   "source": [
    "# Compress some weights with variational scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### sample weights with compression algorithm\n",
    "from rec.beamsearch.Coders.Encoder_Variational import Encoder\n",
    "from rec.beamsearch.distributions.CodingSampler import CodingSampler\n",
    "from rec.beamsearch.distributions.VariationalPosterior import VariationalPosterior\n",
    "from rec.beamsearch.samplers.GreedySampling import GreedySampler\n",
    "from rec.OptimisingVars.VariationalOptimiser import VariationalOptimiser\n",
    "from tqdm.notebook import trange\n",
    "coding_sampler = CodingSampler\n",
    "auxiliary_posterior = VariationalPosterior\n",
    "selection_sampler = GreedySampler\n",
    "omega = 5\n",
    "\n",
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.\n",
    "\n",
    "\n",
    "\n",
    "compute_params_enc =  encoder = Encoder(variational_posterior,\n",
    "                      initial_seed,\n",
    "                      coding_sampler,\n",
    "                      selection_sampler,\n",
    "                      auxiliary_posterior,\n",
    "                      omega,\n",
    "                      epsilon=epsilon,\n",
    "                      beamwidth=beamwidth,\n",
    "                      prior_var=1./alpha)\n",
    "\n",
    "n_auxiliaries = compute_params_enc.n_auxiliary\n",
    "kl_q_p = compute_params_enc.total_kl\n",
    "var_opt = VariationalOptimiser(compute_params_enc.target, omega, n_auxiliaries, kl_q_p, n_trajectories=64, total_var=1./alpha)\n",
    "aux_vars = var_opt.run_optimiser(epochs=5000, lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.\n",
    "\n",
    "compressed_weights_low_eps = []\n",
    "num_compressed_samples = 50\n",
    "for i in trange(num_compressed_samples):\n",
    "    initial_seed = initial_seed + i * 10\n",
    "    encoder = Encoder(variational_posterior,\n",
    "                      initial_seed,\n",
    "                      coding_sampler,\n",
    "                      selection_sampler,\n",
    "                      auxiliary_posterior,\n",
    "                      omega,\n",
    "                      epsilon=epsilon,\n",
    "                      beamwidth=beamwidth,\n",
    "                      prior_var=1./alpha)\n",
    "    \n",
    "    encoder.auxiliary_posterior.coding_sampler.auxiliary_vars = aux_vars\n",
    "    w, idx = encoder.run_encoder()\n",
    "\n",
    "    compressed_weights_low_eps.append(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30140ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list_var_low_eps, log_probs_f = hamiltorch.predict_model(net, x = xs.to(device),\n",
    "                                                  y = xs.to(device), samples=compressed_weights_low_eps,\n",
    "                                                  model_loss=model_loss, tau_out=tau_out,\n",
    "                                                  tau_list=tau_list)\n",
    "\n",
    "plot_preds(pred_list_var_low_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d077f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.1\n",
    "\n",
    "compressed_weights_var_med_eps = []\n",
    "num_compressed_samples = 50\n",
    "for i in trange(num_compressed_samples):\n",
    "    initial_seed = initial_seed + i * 10\n",
    "    encoder = Encoder(variational_posterior,\n",
    "                      initial_seed,\n",
    "                      coding_sampler,\n",
    "                      selection_sampler,\n",
    "                      auxiliary_posterior,\n",
    "                      omega,\n",
    "                      epsilon=epsilon,\n",
    "                      beamwidth=beamwidth,\n",
    "                      prior_var=1./alpha)\n",
    "    \n",
    "    encoder.auxiliary_posterior.coding_sampler.auxiliary_vars = aux_vars\n",
    "    w, idx = encoder.run_encoder()\n",
    "\n",
    "    compressed_weights_var_med_eps.append(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list_var_med_eps, log_probs_f = hamiltorch.predict_model(net, x = xs.to(device),\n",
    "                                                  y = xs.to(device), samples=compressed_weights_var_med_eps,\n",
    "                                                  model_loss=model_loss, tau_out=tau_out,\n",
    "                                                  tau_list=tau_list)\n",
    "plot_preds(pred_list_var_med_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.2\n",
    "\n",
    "compressed_weights_var_high_eps = []\n",
    "num_compressed_samples = 50\n",
    "for i in trange(num_compressed_samples):\n",
    "    initial_seed = initial_seed + i * 10\n",
    "    encoder = Encoder(variational_posterior,\n",
    "                      initial_seed,\n",
    "                      coding_sampler,\n",
    "                      selection_sampler,\n",
    "                      auxiliary_posterior,\n",
    "                      omega,\n",
    "                      epsilon=epsilon,\n",
    "                      beamwidth=beamwidth,\n",
    "                      prior_var=1./alpha)\n",
    "    \n",
    "    encoder.auxiliary_posterior.coding_sampler.auxiliary_vars = aux_vars\n",
    "    w, idx = encoder.run_encoder()\n",
    "\n",
    "    compressed_weights_var_high_eps.append(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list_var_high_eps, log_probs_f = hamiltorch.predict_model(net, x = xs.to(device),\n",
    "                                                  y = xs.to(device), samples=compressed_weights_var_high_eps,\n",
    "                                                  model_loss=model_loss, tau_out=tau_out,\n",
    "                                                  tau_list=tau_list)\n",
    "plot_preds(pred_list_var_high_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e83c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_empirical_predictive(weights, x_input, y_output):\n",
    "    pred_list, _ = hamiltorch.predict_model(net, x = x_input.flatten().to(device),\n",
    "                                                  y = y_output.flatten().to(device), samples=weights,\n",
    "                                                  model_loss='regression', tau_out=tau_out,\n",
    "                                                  tau_list=tau_list)\n",
    "    \n",
    "    # need to make gmm at each sample\n",
    "    return pred_list\n",
    "\n",
    "def make_empirical_gmm(preds):\n",
    "    mix = D.Categorical(torch.ones(preds.shape[0]))\n",
    "    comp = D.Normal(loc=preds.squeeze().permute(1, 0), scale=beta ** -0.5)\n",
    "    gmm = D.MixtureSameFamily(mix, comp)\n",
    "    return gmm\n",
    "\n",
    "def compute_gmm_lp(weights, x, y):\n",
    "    preds = make_empirical_predictive(weights, x, y)\n",
    "    \n",
    "    gmm = make_empirical_gmm(preds)\n",
    "    \n",
    "    return gmm.log_prob(y.squeeze()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_gmm_lp(variational_samples, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_gmm_lp(compressed_weights_low_eps, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_gmm_lp(compressed_weights_var_med_eps, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f81aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_gmm_lp(compressed_weights_var_high_eps, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2387f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the important stuff\n",
    "import pickle as pkl\n",
    "pkl.dump(kl_q_p, open(f'PickledStuff/BNN_STUFF/VAR/var_kl_beta_{ELBO_BETA}.pkl', 'wb'))\n",
    "pkl.dump(variational_samples, open(f'PickledStuff/BNN_STUFF/VAR/var_exact_beta_{ELBO_BETA}.pkl', 'wb'))\n",
    "pkl.dump(compressed_weights_low_eps, open(f'PickledStuff/BNN_STUFF/VAR/var_beta_{ELBO_BETA}_eps_0.pkl', 'wb'))\n",
    "pkl.dump(compressed_weights_var_med_eps, open(f'PickledStuff/BNN_STUFF/VAR/var_beta_{ELBO_BETA}_eps_0.1.pkl', 'wb'))\n",
    "pkl.dump(compressed_weights_var_high_eps, open(f'PickledStuff/BNN_STUFF/VAR/var_beta_{ELBO_BETA}_eps_0.2.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
