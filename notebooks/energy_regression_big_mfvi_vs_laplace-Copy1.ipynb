{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1aa6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/km817/iREC\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953d6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# !wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\" --no-check-certificate\n",
    "data = pd.read_excel('ENB2012_data.xlsx', header=0).iloc[:, :10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "240997c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "from torch import nn\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import HMC, MCMC, SVI, NUTS, TraceMeanField_ELBO\n",
    "from pyro import poutine\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import trange\n",
    "from rec.utils import kl_estimate_with_mc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.distributions as D\n",
    "from torch.nn.utils import parameters_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2026a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Laplace_GCN_Code.preds.laplace import Laplace\n",
    "from Laplace_GCN_Code.preds.likelihoods import GaussianLh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83c84bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = data[:, :-2]\n",
    "y_ = data[:, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f476367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_splits_idxs = []\n",
    "for d in range(x_.shape[-1]):\n",
    "    sorted_x = np.argsort(x_[:,d], axis=-1)\n",
    "    total_points = sorted_x.shape[0]\n",
    "    lower_third = total_points // 3\n",
    "    upper_third = total_points * 2 // 3\n",
    "    test_index = sorted_x[lower_third: upper_third]\n",
    "    test_splits_idxs.append(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d6b102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_splits_x, test_splits_y = [], []\n",
    "train_splits_x, train_splits_y = [], []\n",
    "for d in range(x_.shape[-1]):\n",
    "    a = np.arange(x_.shape[0])\n",
    "    test_index = test_splits_idxs[d]\n",
    "    train_index = np.delete(a, test_index, axis=0)\n",
    "    x_train = x_[train_index]\n",
    "    y_train = y_[train_index]\n",
    "    x_test = x_[test_index][:]\n",
    "    y_test = y_[test_index][:]\n",
    "    x_m = x_train.mean(0)\n",
    "    x_s = x_train.std(0)\n",
    "    x_train = (x_train - x_m) / x_s\n",
    "    x_test = (x_test - x_m) / x_s\n",
    "    test_splits_x.append(x_test)\n",
    "    test_splits_y.append(y_test)\n",
    "    train_splits_x.append(x_train)\n",
    "    train_splits_y.append(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d2b3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = x_train.shape[-1]\n",
    "D_out = y_test.shape[-1]\n",
    "x_train = torch.FloatTensor(np.array(train_splits_x))\n",
    "y_train = torch.FloatTensor(np.array(train_splits_y))\n",
    "x_test= torch.FloatTensor(np.array(test_splits_x))\n",
    "y_test = torch.FloatTensor(np.array(test_splits_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dfbdd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model(x, y=None, weight_samples=None, in_size=1, num_nodes=10, out_size=1, ELBO_BETA=1.):\n",
    "    # sample vector of weights for regression\n",
    "    total_weights = (in_size + 1) * num_nodes + (num_nodes + 1) * num_nodes + (num_nodes + 1) * out_size\n",
    "    # sample params\n",
    "    with poutine.scale(scale=ELBO_BETA):\n",
    "        params = pyro.sample(\"params\", dist.Normal(torch.zeros(total_weights + D_out), 1.).to_event(1))\n",
    "    weights, rho = params[:-D_out], params[-D_out:]\n",
    "\n",
    "    idx = 0\n",
    "    fc1_weights = weights[idx: idx + in_size * num_nodes].reshape(num_nodes, in_size)\n",
    "    idx += in_size * num_nodes\n",
    "    fc1_bias = weights[idx: idx + num_nodes].reshape(num_nodes)\n",
    "    idx += num_nodes\n",
    "\n",
    "    fc2_weights = weights[idx: idx + num_nodes * num_nodes].reshape(num_nodes, num_nodes)\n",
    "    idx += num_nodes * num_nodes\n",
    "    fc2_bias = weights[idx: idx + num_nodes].reshape(num_nodes)\n",
    "    idx += num_nodes\n",
    "\n",
    "    fc3_weights = weights[idx: idx + num_nodes * out_size].reshape(out_size, num_nodes)\n",
    "    idx += num_nodes * out_size\n",
    "    fc3_bias = weights[idx: idx + out_size].reshape(out_size)\n",
    "    idx += out_size\n",
    "\n",
    "    assert idx == total_weights, \"Something wrong with number of weights!\"\n",
    "\n",
    "    # compute forward pass\n",
    "    batch_shape = x.shape[0]\n",
    "    x = torch.einsum(\"ij, kj -> ki\", fc1_weights, x) + fc1_bias[None].repeat(batch_shape, 1)\n",
    "    x = torch.relu(x)\n",
    "\n",
    "    x = torch.einsum(\"ij, kj -> ki\", fc2_weights, x) + fc2_bias[None].repeat(batch_shape, 1)\n",
    "    x = torch.relu(x)\n",
    "\n",
    "    x = torch.einsum(\"ij, kj -> ki\", fc3_weights, x) + fc3_bias[None].repeat(batch_shape, 1)\n",
    "    mu = x.squeeze()\n",
    "\n",
    "    with pyro.plate(\"data\"):\n",
    "        obs = pyro.sample(\"obs\", dist.MultivariateNormal(loc=mu, \n",
    "                                                         covariance_matrix=torch.diag(F.softplus(rho) ** 2)), obs=y)\n",
    "    return mu\n",
    "\n",
    "def make_empirical_gmm(samples, num_nodes, x_test):\n",
    "    rho_noise = samples['params'][:, -D_out:]\n",
    "    noise = F.softplus(rho_noise) ** 2\n",
    "    preds_dict = Predictive(regression_model, samples, return_sites=['_RETURN'])(x_test, None, num_nodes=num_nodes,\n",
    "                                                                                 in_size=D_in, out_size=D_out)\n",
    "    preds = preds_dict['_RETURN']\n",
    "    mix = dist.Categorical(torch.ones(preds.shape[0]))\n",
    "    comp = dist.MultivariateNormal(loc=preds.squeeze().permute(1, 0, 2), covariance_matrix=torch.diag_embed(noise))\n",
    "    gmm = dist.MixtureSameFamily(mix, comp)\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a94f72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class map_regression_model(nn.Module):\n",
    "    def __init__(self, in_size=1, num_nodes=10, out_size=1):\n",
    "        super(map_regression_model, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.activation = torch.relu\n",
    "        self.num_nodes = num_nodes\n",
    "\n",
    "        self.fc1_weights = nn.Parameter(torch.randn(self.num_nodes, self.in_size))\n",
    "        self.fc1_bias = nn.Parameter(torch.randn(self.num_nodes))\n",
    "\n",
    "        self.fc2_weights = nn.Parameter(torch.randn(self.num_nodes, self.num_nodes))\n",
    "        self.fc2_bias = nn.Parameter(torch.randn(self.num_nodes))\n",
    "\n",
    "        self.fc3_weights = nn.Parameter(torch.randn(self.out_size, self.num_nodes))\n",
    "        self.fc3_bias = nn.Parameter(torch.randn(self.out_size))\n",
    "\n",
    "        self.rho = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    # compute forward pass\n",
    "    def forward(self, x):\n",
    "        batch_shape = x.shape[0]\n",
    "        x = torch.einsum(\"ij, kj -> ki\", self.fc1_weights, x) + self.fc1_bias[None].repeat(batch_shape, 1)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = torch.einsum(\"ij, kj -> ki\", self.fc2_weights, x) + self.fc2_bias[None].repeat(batch_shape, 1)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = torch.einsum(\"ij, kj -> ki\", self.fc3_weights, x) + self.fc3_bias[None].repeat(batch_shape, 1)\n",
    "        x = x.squeeze()\n",
    "\n",
    "        return x  \n",
    "    \n",
    "    def loss_function(self, x, y):\n",
    "        y_preds = self.forward(x)\n",
    "        return -D.Normal(loc=y_preds, scale=F.softplus(self.rho)).log_prob(y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e035560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class deterministic_regression_model(nn.Module):\n",
    "    def __init__(self, params, in_size=1, num_nodes=10, out_size=1):\n",
    "        super(deterministic_regression_model, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.activation = torch.relu\n",
    "        self.num_nodes = num_nodes\n",
    "        weights, rho = params[:-out_size], params[-out_size:]\n",
    "\n",
    "        idx = 0\n",
    "        self.fc1_weights = weights[idx: idx + self.in_size * self.num_nodes].reshape(self.num_nodes, self.in_size)\n",
    "        idx += self.in_size * self.num_nodes\n",
    "        self.fc1_bias = weights[idx: idx + self.num_nodes].reshape(self.num_nodes)\n",
    "        idx += self.num_nodes\n",
    "\n",
    "        self.fc2_weights = weights[idx: idx + self.num_nodes * self.num_nodes].reshape(self.num_nodes, self.num_nodes)\n",
    "        idx += self.num_nodes * self.num_nodes\n",
    "        self.fc2_bias = weights[idx: idx + self.num_nodes].reshape(self.num_nodes)\n",
    "        idx += self.num_nodes\n",
    "\n",
    "        self.fc3_weights = weights[idx: idx + self.num_nodes *self.out_size].reshape(self.out_size, self.num_nodes)\n",
    "        idx += self.num_nodes *self.out_size\n",
    "        self.fc3_bias = weights[idx: idx +self.out_size].reshape(self.out_size)\n",
    "        idx +=self.out_size\n",
    "        \n",
    "        self.weights = weights\n",
    "        self.rho = rho\n",
    "        self.params = params\n",
    "\n",
    "        # compute forward pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_shape = x.shape[0]\n",
    "        x = torch.einsum(\"ij, kj -> ki\", self.fc1_weights, x) + self.fc1_bias[None].repeat(batch_shape, 1)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = torch.einsum(\"ij, kj -> ki\", self.fc2_weights, x) + self.fc2_bias[None].repeat(batch_shape, 1)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = torch.einsum(\"ij, kj -> ki\", self.fc3_weights, x) + self.fc3_bias[None].repeat(batch_shape, 1)\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def weight_prior_lp(self):\n",
    "        return dist.Normal(loc=0., scale=1.).log_prob(self.params).mean()\n",
    "    \n",
    "    def data_likelihood(self, x, y):\n",
    "        likelihood = dist.Normal(loc=self.forward(x),\n",
    "                              scale=F.softplus(self.rho))\n",
    "        return likelihood.log_prob(y).sum(-1).mean()\n",
    "    \n",
    "    def joint_log_prob(self, x, y):\n",
    "        return self.data_likelihood(x, y) + self.weight_prior_lp(x, y)\n",
    "    \n",
    "    def make_weights_from_sample(self, params):\n",
    "        weights, rho = params[:-self.out_size], params[-self.out_size:]\n",
    "\n",
    "        idx = 0\n",
    "        self.fc1_weights = weights[idx: idx + self.in_size * self.num_nodes].reshape(self.num_nodes, self.in_size)\n",
    "        idx += self.in_size * self.num_nodes\n",
    "        self.fc1_bias = weights[idx: idx + self.num_nodes].reshape(self.num_nodes)\n",
    "        idx += self.num_nodes\n",
    "\n",
    "        self.fc2_weights = weights[idx: idx + self.num_nodes * self.num_nodes].reshape(self.num_nodes, self.num_nodes)\n",
    "        idx += self.num_nodes * self.num_nodes\n",
    "        self.fc2_bias = weights[idx: idx + self.num_nodes].reshape(self.num_nodes)\n",
    "        idx += self.num_nodes\n",
    "\n",
    "        self.fc3_weights = weights[idx: idx + self.num_nodes * self.out_size].reshape(self.out_size, self.num_nodes)\n",
    "        idx += self.num_nodes *self.out_size\n",
    "        self.fc3_bias = weights[idx: idx + self.out_size].reshape(self.out_size)\n",
    "        idx += self.out_size\n",
    "        \n",
    "        self.weights = weights\n",
    "        self.rho = rho\n",
    "        self.params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5921015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(10)\n",
    "ELBO_BETA = 1.\n",
    "S=1\n",
    "in_size = x_train.shape[-1]\n",
    "num_nodes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d60528",
   "metadata": {},
   "source": [
    "# Train MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d4c35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Laplace_GCN_Code.preds.models import MLPS\n",
    "\n",
    "model = MLPS(input_size=D_in, hidden_sizes=[num_nodes]*1, output_size=D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b78d67db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class homo_noise_model(nn.Module):\n",
    "    def __init__(self, old_model):\n",
    "        super(homo_noise_model, self).__init__()\n",
    "        self.oldmodel = old_model\n",
    "        self.rho = nn.Parameter(torch.randn(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "845887f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w_noise = homo_noise_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3002f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bf5453e0ee4b62af4ce203a9fee843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optim = torch.optim.Adam(model_w_noise.parameters(), lr=1e-2)\n",
    "prior_prec = 1\n",
    "losses = list()\n",
    "num_iterations = 10000\n",
    "pbar = trange(num_iterations)\n",
    "for j in pbar:\n",
    "    f = model(x_train[S])\n",
    "    w = parameters_to_vector(model_w_noise.parameters())\n",
    "    reg = 0.5 * prior_prec * w @ w\n",
    "    lh = GaussianLh(sigma_noise=F.softplus(model_w_noise.rho))\n",
    "    loss = - lh.log_likelihood(y_train[S], f) + reg\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    losses.append(loss.item())\n",
    "    pbar.set_description(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(x_train)))\n",
    "    model_w_noise.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "959ba408",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh = GaussianLh(F.softplus(model_w_noise.rho.detach()))\n",
    "posterior = Laplace(model, prior_prec, lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "35075e1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-da182a60f296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mposterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/km817/iREC/Laplace_GCN_Code/preds/laplace.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, train_loader, cov_type, dampen_kron)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mJs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGGN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mprecision\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mpk,mkl,mql->pq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mChol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/km817/iREC/Laplace_GCN_Code/preds/optimizers.py\u001b[0m in \u001b[0;36mGGN\u001b[0;34m(model, likelihood, data, target, ret_f)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mHess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/km817/iREC/Laplace_GCN_Code/preds/likelihoods.py\u001b[0m in \u001b[0;36mHessian\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mHessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_2_dispersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "posterior.infer(train_loader, cov_type='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e61b19f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9386e-05, 3.9388e-05, 3.0950e-05, 5.1048e-05, 5.4764e-05, 5.4762e-05,\n",
       "        5.4316e-05, 5.2974e-05, 7.4291e-05, 8.0182e-05, 1.3283e-04, 7.4850e-05,\n",
       "        8.6408e-05, 8.6406e-05, 5.8138e-05, 5.8095e-05, 4.5158e-04, 2.9369e-04,\n",
       "        3.0561e-04, 2.9078e-04, 1.3864e-04, 1.3864e-04, 1.3897e-04, 1.3776e-04,\n",
       "        5.4764e-05, 8.6408e-05, 1.3864e-04, 4.4131e-03, 4.0901e-03, 3.4409e-03,\n",
       "        4.4131e-03, 4.0901e-03, 3.4409e-03, 2.7968e-03, 2.7968e-03])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "192a95dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.1666e-01,  2.0569e-01, -1.1963e+00,  5.5990e-01, -1.7350e+00,\n",
       "        -6.8464e-03, -1.1957e-01,  2.8761e-02,  1.2986e+00,  9.8996e-01,\n",
       "         1.5477e-01,  1.1111e+00,  6.2162e-01, -5.9374e-03,  5.9999e-01,\n",
       "         6.5159e-02,  1.9713e+00, -8.4253e-01, -2.2525e+00, -3.7284e-01,\n",
       "        -4.1940e+00,  8.4336e-03,  5.6838e-02, -2.7724e-02, -4.4291e-02,\n",
       "         1.4071e+00,  2.9631e+00, -1.0842e+01,  1.0745e+01,  8.6865e+00,\n",
       "        -1.0073e+01,  7.8678e+00,  8.9527e+00,  5.3750e+00,  9.9558e+00])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "04f88cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([ 5201.3369,  6981.8086, 24860.2207]),\n",
       "  tensor([0.0000e+00, 1.0324e-03, 5.8550e-02, 6.3212e-01, 7.8704e-01, 1.0000e+00,\n",
       "          1.2130e+00, 4.3083e+00])],\n",
       " [tensor([ 5201.3369,  6981.8086, 24860.2207])],\n",
       " [tensor([356.5528, 356.5528]), tensor([0.2181, 0.6611, 1.2487])],\n",
       " [tensor([356.5528, 356.5528])]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior.Sigma.Lams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef704eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.1666e-01,  2.0569e-01, -1.1963e+00,  5.5990e-01, -1.7350e+00,\n",
       "        -6.8464e-03, -1.1957e-01,  2.8761e-02,  1.2986e+00,  9.8996e-01,\n",
       "         1.5477e-01,  1.1111e+00,  6.2162e-01, -5.9374e-03,  5.9999e-01,\n",
       "         6.5159e-02,  1.9713e+00, -8.4253e-01, -2.2525e+00, -3.7284e-01,\n",
       "        -4.1940e+00,  8.4336e-03,  5.6838e-02, -2.7724e-02, -4.4291e-02,\n",
       "         1.4071e+00,  2.9631e+00, -1.0842e+01,  1.0745e+01,  8.6865e+00,\n",
       "        -1.0073e+01,  7.8678e+00,  8.9527e+00,  5.3750e+00,  9.9558e+00],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_to_vector(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1874d464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.6167,  0.2057, -1.1963,  0.5599, -1.7350, -0.0068, -0.1196,  0.0288],\n",
       "         [ 1.2986,  0.9900,  0.1548,  1.1111,  0.6216, -0.0059,  0.6000,  0.0652],\n",
       "         [ 1.9713, -0.8425, -2.2525, -0.3728, -4.1940,  0.0084,  0.0568, -0.0277]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0443,  1.4071,  2.9631], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-10.8421,  10.7447,   8.6865],\n",
       "         [-10.0729,   7.8678,   8.9527]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([5.3750, 9.9558], requires_grad=True)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = pyro.optim.Adam({\"lr\": 1e-2})\n",
    "\n",
    "# train Factored Gaussian approx\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(regression_model)\n",
    "svi = SVI(regression_model, guide, optimizer, loss=TraceMeanField_ELBO())\n",
    "num_iterations = 20000\n",
    "pyro.clear_param_store()\n",
    "pbar = trange(num_iterations)\n",
    "losses = []\n",
    "for j in pbar:\n",
    "    # calculate the loss and take a gradient step\n",
    "    loss = svi.step(x_train[S], y_train[S], ELBO_BETA=ELBO_BETA, num_nodes=num_nodes, in_size=D_in, out_size=D_out)\n",
    "    losses.append(loss)\n",
    "    pbar.set_description(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(x_train)))\n",
    "guide.requires_grad_(False)\n",
    "\n",
    "params = []\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    params.append(pyro.param(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e1e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab2f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "means, stds = params\n",
    "variational_posterior = dist.MultivariateNormal(loc=means, covariance_matrix=torch.diag(stds ** 2))\n",
    "variational_sample = variational_posterior.sample((50,))\n",
    "variational_samples = {\"params\" : variational_sample}\n",
    "kl_var_prior = kl_estimate_with_mc(variational_posterior, prior)\n",
    "var_pred = Predictive(regression_model, variational_samples, return_sites=['obs', '_RETURN'])(x_test[S], None, \n",
    "                                                                        num_nodes=num_nodes, in_size=D_in,\n",
    "                                                                                             out_size=D_out)\n",
    "VAR_RMSE = ((var_pred['_RETURN'].mean(0) - y_test[S]) ** 2).mean().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "var_gmm = make_empirical_gmm(variational_samples, num_nodes, x_test[S])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19384eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_gmm.log_prob(y_test[S]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_gmm = make_empirical_gmm(full_samples, num_nodes, x_test[S])\n",
    "kde_gmm = make_empirical_gmm(kde_samples, num_nodes, x_test[S])\n",
    "var_gmm = make_empirical_gmm(variational_samples, num_nodes, x_test[S])\n",
    "print(f\"The final KLs are: KDE {kl_kde_prior}, VAR {kl_var_prior}\\n\"\n",
    "      f\"The final RMSE are: HMC {HMC_RMSE}, KDE {KDE_RMSE}, VAR {VAR_RMSE}\\n\"\n",
    "      f\"The final LLs are: HMC {hmc_gmm.log_prob(y_test[S]).mean()}, KDE {kde_gmm.log_prob(y_test[S]).mean()}, VAR {var_gmm.log_prob(y_test[S]).mean()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_gmm = make_empirical_gmm(full_samples, num_nodes, x_train[S])\n",
    "kde_gmm = make_empirical_gmm(kde_samples, num_nodes, x_train[S])\n",
    "var_gmm = make_empirical_gmm(variational_samples, num_nodes, x_train[S])\n",
    "print(f\"The final KLs are: KDE {kl_kde_prior}, VAR {kl_var_prior}\\n\"\n",
    "      f\"The final RMSE are: HMC {HMC_RMSE}, KDE {KDE_RMSE}, VAR {VAR_RMSE}\\n\"\n",
    "      f\"The final LLs are: HMC {hmc_gmm.log_prob(y_train[S]).mean()}, KDE {kde_gmm.log_prob(y_train[S]).mean()}, VAR {var_gmm.log_prob(y_train[S]).mean()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a0740",
   "metadata": {},
   "source": [
    "# Compress weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd607f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets compress some samples\n",
    "#### sample weights with compression algorithm\n",
    "from tqdm.notebook import trange\n",
    "from rec.beamsearch.Coders.Encoder_Empirical import Encoder\n",
    "from rec.beamsearch.distributions.CodingSampler import CodingSampler\n",
    "from rec.beamsearch.distributions.EmpiricalMixturePosterior import EmpiricalMixturePosterior\n",
    "from rec.beamsearch.samplers.GreedySampling_BNNs import GreedySampler\n",
    "import pyro.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f50d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = deterministic_regression_model(full_samples['params'][10], in_size=D_in, num_nodes=num_nodes, out_size=D_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20464c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = full_samples['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2cfde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_q_p = kl_kde_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rec.OptimisingVars.FinalJointOptimiser import FinalJointOptimiser\n",
    "coding_sampler = CodingSampler\n",
    "auxiliary_posterior = EmpiricalMixturePosterior\n",
    "selection_sampler = GreedySampler\n",
    "omega = 5\n",
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.\n",
    "dummy_encoder = Encoder(dummy_model,\n",
    "                     x_train[S],\n",
    "                     y_train[S],\n",
    "                     samples,\n",
    "                     initial_seed,\n",
    "                     coding_sampler,\n",
    "                     selection_sampler,\n",
    "                     auxiliary_posterior,\n",
    "                     omega,\n",
    "                     beamwidth,\n",
    "                     epsilon=epsilon,\n",
    "                     prior_var=1.,\n",
    "                     total_kl=kl_q_p)\n",
    "\n",
    "z_sample = samples.mean(0)\n",
    "omega = 5\n",
    "n_trajectories = 64\n",
    "n_auxiliaries = dummy_encoder.n_auxiliary\n",
    "prior_var = 1.\n",
    "emp_opt = FinalJointOptimiser(z_sample, omega, n_auxiliaries, kl_q_p, n_trajectories, prior_var)\n",
    "aux_vars = emp_opt.run_optimiser(epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d14d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dummy_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bec8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_sampler = CodingSampler\n",
    "auxiliary_posterior = EmpiricalMixturePosterior\n",
    "selection_sampler = GreedySampler\n",
    "omega = 5\n",
    "\n",
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.2\n",
    "num_compressed_samples = 500\n",
    "compressed_weights_emp_low_eps = []\n",
    "\n",
    "for i in trange(num_compressed_samples):\n",
    "    initial_seed = initial_seed + i * 10\n",
    "    encoder = Encoder(dummy_model,\n",
    "                     x_train[S],\n",
    "                     y_train[S],\n",
    "                     samples,\n",
    "                     initial_seed,\n",
    "                     coding_sampler,\n",
    "                     selection_sampler,\n",
    "                     auxiliary_posterior,\n",
    "                     omega,\n",
    "                     beamwidth,\n",
    "                     epsilon=epsilon,\n",
    "                     prior_var=1.,\n",
    "                     total_kl=kl_q_p)\n",
    "    \n",
    "    encoder.auxiliary_posterior.coding_sampler.auxiliary_vars = aux_vars\n",
    "    \n",
    "    w, idx = encoder.run_encoder()\n",
    "    compressed_weights_emp_low_eps.append(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_samples = torch.zeros([0])\n",
    "for w in compressed_weights_emp_low_eps:\n",
    "    weight_samples = torch.cat([weight_samples, w[None]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890686b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_samples = {'params':weight_samples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1175cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = Predictive(regression_model, weight_samples, return_sites=['obs', '_RETURN'])(x_test[S], None, \n",
    "                                                                        num_nodes=num_nodes, in_size=D_in,\n",
    "                                                                                             out_size=D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c41881",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_gmm = make_empirical_gmm(weight_samples, num_nodes, x_test[S])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71553a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_gmm.log_prob(y_test[S]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298483e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rec.beamsearch.distributions.CodingSampler import CodingSampler\n",
    "from rec.beamsearch.distributions.KDEPosterior import KDEPosterior\n",
    "from rec.beamsearch.samplers.GreedySampling_BNNs import GreedySampler\n",
    "from rec.beamsearch.Coders.Encoder_KDE_BNN import EncoderKDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "KDE_target = kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a4f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_sampler = CodingSampler\n",
    "auxiliary_posterior = KDEPosterior\n",
    "selection_sampler = GreedySampler\n",
    "omega = 5\n",
    "\n",
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.2\n",
    "num_compressed_samples = 500\n",
    "compressed_weights_kde_low_eps = []\n",
    "\n",
    "for i in trange(num_compressed_samples):\n",
    "    initial_seed = initial_seed + i * 10\n",
    "    encoder = EncoderKDE(model=dummy_model,\n",
    "                         x_data=x_train[S],\n",
    "                         y_data=y_train[S],\n",
    "                         target=KDE_target,\n",
    "                         initial_seed=initial_seed,\n",
    "                         coding_sampler=coding_sampler,\n",
    "                         selection_sampler=selection_sampler,\n",
    "                         auxiliary_posterior=auxiliary_posterior,\n",
    "                         omega=omega,\n",
    "                         epsilon=epsilon,\n",
    "                         beamwidth=beamwidth,\n",
    "                         prior_var=1.,\n",
    "                        total_kl=kl_q_p)\n",
    "    \n",
    "    encoder.auxiliary_posterior.coding_sampler.auxiliary_vars = aux_vars\n",
    "    \n",
    "    w, idx = encoder.run_encoder()\n",
    "    compressed_weights_kde_low_eps.append(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_samples_kde = torch.zeros([0])\n",
    "for w in compressed_weights_kde_low_eps:\n",
    "    weight_samples_kde = torch.cat([weight_samples_kde, w[None]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffea139",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs_kde = torch.zeros([0])\n",
    "for w in compressed_weights_kde_low_eps:\n",
    "    dummy_model.make_weights_from_sample(w)\n",
    "    log_probs_kde = torch.cat([log_probs_kde, dummy_model.data_likelihood(x_train[S], y_train[S])[None]], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192855a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\"params\":weight_samples_kde[torch.topk(log_probs_kde, k=25)[1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_samples_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_samples_kde = {'params':weight_samples_kde}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = Predictive(regression_model, weight_samples_kde, return_sites=['obs', '_RETURN'])(x_test[S], None, \n",
    "                                                                        num_nodes=num_nodes, in_size=D_in,\n",
    "                                                                                             out_size=D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_gmm_kde = make_empirical_gmm(weight_samples_kde, num_nodes, x_test[S])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb80562",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_gmm_kde.log_prob(y_test[S]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d58aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_gmm_kde_train = make_empirical_gmm(weight_samples_kde, num_nodes, x_train[S])\n",
    "\n",
    "compressed_gmm_kde_train.log_prob(y_train[S]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbaf5c0",
   "metadata": {},
   "source": [
    "# Variational Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### sample weights with compression algorithm\n",
    "from rec.beamsearch.Coders.Encoder_Variational import Encoder\n",
    "from rec.beamsearch.distributions.CodingSampler import CodingSampler\n",
    "from rec.beamsearch.distributions.VariationalPosterior import VariationalPosterior\n",
    "from rec.beamsearch.samplers.GreedySampling import GreedySampler\n",
    "from rec.OptimisingVars.VariationalOptimiser import VariationalOptimiser\n",
    "from tqdm.notebook import trange\n",
    "coding_sampler = CodingSampler\n",
    "auxiliary_posterior = VariationalPosterior\n",
    "selection_sampler = GreedySampler\n",
    "omega = 5\n",
    "\n",
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.\n",
    "\n",
    "\n",
    "\n",
    "compute_params_enc =  encoder = Encoder(variational_posterior,\n",
    "                      initial_seed,\n",
    "                      coding_sampler,\n",
    "                      selection_sampler,\n",
    "                      auxiliary_posterior,\n",
    "                      omega,\n",
    "                      epsilon=epsilon,\n",
    "                      beamwidth=beamwidth,\n",
    "                      prior_var=1.)\n",
    "\n",
    "n_auxiliaries = compute_params_enc.n_auxiliary\n",
    "kl_q_p = compute_params_enc.total_kl\n",
    "var_opt = VariationalOptimiser(compute_params_enc.target, omega, n_auxiliaries, kl_q_p, n_trajectories=16, total_var=1.)\n",
    "aux_vars = var_opt.run_optimiser(epochs=1000, lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1322371",
   "metadata": {},
   "outputs": [],
   "source": [
    "del compute_params_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdcff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_seed = 0\n",
    "beamwidth = 1\n",
    "epsilon = 0.2\n",
    "\n",
    "compressed_weights_var_high_eps = []\n",
    "num_compressed_samples = 500\n",
    "for i in trange(num_compressed_samples):\n",
    "    initial_seed = initial_seed + i * 10\n",
    "    encoder = Encoder(variational_posterior,\n",
    "                      initial_seed,\n",
    "                      coding_sampler,\n",
    "                      selection_sampler,\n",
    "                      auxiliary_posterior,\n",
    "                      omega,\n",
    "                      epsilon=epsilon,\n",
    "                      beamwidth=beamwidth,\n",
    "                      prior_var=1.)\n",
    "    \n",
    "    encoder.auxiliary_posterior.coding_sampler.auxiliary_vars = aux_vars\n",
    "    w, idx = encoder.run_encoder()\n",
    "\n",
    "    compressed_weights_var_high_eps.append(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_samples_var = torch.zeros([0])\n",
    "for w in compressed_weights_var_high_eps:\n",
    "    weight_samples_var = torch.cat([weight_samples_var, w[None]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a543bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_samples_var = {'params':weight_samples_var}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da278808",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = Predictive(regression_model, weight_samples_var, return_sites=['obs', '_RETURN'])(x_test[S], None, \n",
    "                                                                        num_nodes=num_nodes, in_size=D_in,\n",
    "                                                                                             out_size=D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bc2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_gmm_var = make_empirical_gmm(weight_samples_var, num_nodes, x_test[S])\n",
    "\n",
    "compressed_gmm_var.log_prob(y_test[S]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a0cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_gmm_var_train = make_empirical_gmm(weight_samples_var, num_nodes, x_train[S])\n",
    "\n",
    "compressed_gmm_var_train.log_prob(y_train[S]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# pkl.dump(full_samples, open(f\"PickledStuff/BNN_UCI/ENERGY/full_samples_split{S}.pkl\", \"wb\"))\n",
    "# pkl.dump(kde_samples, open(f\"PickledStuff/BNN_UCI/ENERGY/kde_samples_split{S}.pkl\", \"wb\"))\n",
    "pkl.dump(variational_samples, open(f\"PickledStuff/BNN_UCI/ENERGY/variational_samples_split{S}.pkl\", \"wb\"))\n",
    "# pkl.dump(weight_samples_kde, open(f\"PickledStuff/BNN_UCI/ENERGY/compressed_kde_samples_split{S}.pkl\", \"wb\"))\n",
    "# pkl.dump(weight_samples, open(f\"PickledStuff/BNN_UCI/ENERGY/compressed_emp_samples_split{S}.pkl\", \"wb\"))\n",
    "pkl.dump(weight_samples_var, open(f\"PickledStuff/BNN_UCI/ENERGY/compressed_var_samples_split{S}.pkl\", \"wb\"))\n",
    "# pkl.dump(kl_kde_prior, open(f\"PickledStuff/BNN_UCI/ENERGY/kde_kl_split{S}.pkl\", \"wb\"))\n",
    "pkl.dump(kl_var_prior, open(f\"PickledStuff/BNN_UCI/ENERGY/var_kl_split{S}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ea80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe4763d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
