{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9735722",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8245d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1d5b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kristophermiltiadou/Documents/UniWork/Cambridge/Thesis/CODE/iREC\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0f5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from models.SimpleBayesianLinRegressor import BayesLinRegressor\n",
    "from rec.utils import compute_variational_posterior\n",
    "\n",
    "import math\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9948cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60977020",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.weight': 'normal'})\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams.update({'lines.linewidth' : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff024cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e64989e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blr_problem(dim, seed):\n",
    "    if dim == 2:\n",
    "        signal_std = 1e-2\n",
    "    else:\n",
    "        signal_std = 1e-1\n",
    "    prior_alpha = 1\n",
    "    num_training = dim\n",
    "    torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "    blr = BayesLinRegressor(prior_mean=torch.zeros(dim),\n",
    "                            prior_alpha=prior_alpha,\n",
    "                            signal_std=signal_std,\n",
    "                            num_targets=2 * num_training,\n",
    "                            seed=seed,\n",
    "                            num_train_points=num_training)\n",
    "    blr.sample_feature_inputs()\n",
    "    blr.sample_regression_targets()\n",
    "    blr.posterior_update()\n",
    "    target = blr.weight_posterior\n",
    "    return blr, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba54c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_preds(d, blr_obj, training_data=False):\n",
    "    preds = torch.zeros([0])\n",
    "    sample_list = torch.tensor(d['compressed_samples'])\n",
    "    for s in sample_list:\n",
    "        preds = torch.cat((preds, blr_obj.empirical_prediction(s, training_data=training_data)[None]))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9fdaa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_preds(blr_obj, training_data=False, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    samples = blr_obj.weight_posterior.sample((500,))\n",
    "    preds = torch.zeros([0])\n",
    "    for s in samples:\n",
    "        preds = torch.cat((preds, blr_obj.empirical_prediction(s, training_data=training_data)[None]))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c15578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_approx_preds(blr_obj, training_data=False, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    samples = compute_variational_posterior(blr_obj.weight_posterior).sample((500,))\n",
    "    preds = torch.zeros([0])\n",
    "    for s in samples:\n",
    "        preds = torch.cat((preds, blr_obj.empirical_prediction(s, training_data=training_data)[None]))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd93447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_prob_empirical_mog(full_preds, blr):\n",
    "    normal_dists = D.normal.Normal(loc=full_preds, scale = signal_std)\n",
    "    log_probs = normal_dists.log_prob(blr.regression_targets_test)\n",
    "    sum_of_probs = torch.logsumexp(log_probs, dim=1)\n",
    "    normalisation_constant = torch.log(torch.tensor(full_preds.shape[1]))\n",
    "    gmm_log_prob_per_point = sum_of_probs - normalisation_constant\n",
    "    gmm_log_prob = torch.sum(gmm_log_prob_per_point, dim=1)\n",
    "    return gmm_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ce8fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gmm(preds, signal_std):\n",
    "    mix = D.Categorical(torch.ones(preds.shape[1]))\n",
    "    comp = D.Normal(loc=preds.permute(0, 2, 1), scale=b.signal_std)\n",
    "    gmm = D.MixtureSameFamily(mix, comp)\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d95a0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gaussian_likelihood(preds, y, b):\n",
    "    # make normal using preds and signal noise\n",
    "    m = preds.mean(1).to('cpu')\n",
    "    s_al = (preds.var(1).to('cpu') + b.signal_std ** 2) ** 0.5\n",
    "    \n",
    "    return D.Normal(loc=m, scale=s_al).log_prob(y).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c94f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_expected_coding_efficiency(kl, epsilon=0.2):\n",
    "    K = (1 + epsilon) * kl\n",
    "    return K + torch.log(K + 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e89b76c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from rec.beamsearch.distributions.CodingSampler import CodingSampler\n",
    "from rec.beamsearch.distributions.EmpiricalMixturePosterior import EmpiricalMixturePosterior\n",
    "from rec.beamsearch.samplers.GreedySampling import GreedySampler\n",
    "from rec.beamsearch.Coders.Encoder import Encoder as Empirical_Encoder\n",
    "from models.SimpleBayesianLinRegressor import BayesLinRegressor\n",
    "from rec.utils import kl_estimate_with_mc, plot_samples_in_2d, plot_running_sum_2d, plot_pairs_of_samples, compute_variational_posterior\n",
    "from rec.OptimisingVars.FinalJointOptimiser import FinalJointOptimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77adccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10\n",
    "eps = 0.3\n",
    "beamwidth = 5\n",
    "b, t = create_blr_problem(dim, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a54adbe",
   "metadata": {},
   "source": [
    "# Empirical Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sample(target, omega=5, epsilon=0.,\n",
    "                  n_empirical_samples=10, seed=10, beamwidth=1, optimising_vars=False, aux_vars=None, dont_run=False):\n",
    "    \n",
    "    encoder = Empirical_Encoder(target,\n",
    "                                seed,\n",
    "                                CodingSampler,\n",
    "                                GreedySampler,\n",
    "                                EmpiricalMixturePosterior,\n",
    "                                omega,\n",
    "                                n_empirical_samples,\n",
    "                                epsilon=epsilon,\n",
    "                                beamwidth=beamwidth\n",
    "                                )\n",
    "    if aux_vars is not None:\n",
    "        encoder.auxiliary_posterior.coding_sampler.auxiliary_vars = aux_vars\n",
    "    \n",
    "    if dont_run:\n",
    "        return encoder\n",
    "    else:\n",
    "        return encoder, *encoder.run_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fff012",
   "metadata": {},
   "outputs": [],
   "source": [
    "pckled_stuff = pkl.load(open(f\"PickledStuff/BLR_RESULTS_v2/Dim{dim}/Empirical_Epsilon{eps}_Beam{beamwidth}_Omega5.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d6d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_vars = pckled_stuff['aux_vars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc, z, idx = encode_sample(t, aux_vars=aux_vars, epsilon=eps, beamwidth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc05aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(target, prior, aux_samples, num_samples=1000):\n",
    "    samples = target.sample((num_samples,))\n",
    "    dim = target.mean.shape[-1]\n",
    "    running_sum = torch.cumsum(aux_samples, dim=0)\n",
    "    final_sample = running_sum[-1]\n",
    "    f, ax = plt.subplots(dim, dim, figsize=(3*dim, 3*dim), sharex=True, sharey=True)\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if i!=j:\n",
    "                ax[i, j].plot(samples[:, i], samples[:, j], 'x')\n",
    "                ax[i, j].plot(running_sum[:, i], running_sum[:, j], 'o-')\n",
    "                ax[i, j].plot(final_sample[i], final_sample[j], 'ko')\n",
    "            if i == dim - 1:\n",
    "                ax[i, j].set_xlabel(f'$\\mathbf{{w}}_{{{j+1}}}$')\n",
    "            if j == 0:\n",
    "                ax[i, j].set_ylabel(f'$\\mathbf{{w}}_{{{i+1}}}$')\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traj(t, b, enc.selected_samples[0])\n",
    "plt.savefig(f\"Figures/Thesis/emp_blr{dim}_eps{eps}_traj.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f97ec7c",
   "metadata": {},
   "source": [
    "# Variational Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rec.beamsearch.distributions.VariationalPosterior import VariationalPosterior\n",
    "from rec.beamsearch.Coders.Encoder_Variational import Encoder as Variational_Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9412935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sample_var(target, omega=5, epsilon=0.,\n",
    "                  n_empirical_samples=10, seed=10, beamwidth=1, optimising_vars=False, aux_vars=None, dont_run=False):\n",
    "    \n",
    "    target = compute_variational_posterior(target)\n",
    "    encoder = Variational_Encoder(target,\n",
    "                                  seed,\n",
    "                                  CodingSampler,\n",
    "                                  GreedySampler,\n",
    "                                  VariationalPosterior,\n",
    "                                  omega,\n",
    "                                  epsilon=epsilon,\n",
    "                                  beamwidth=beamwidth\n",
    "                                  )\n",
    "    if aux_vars is not None:\n",
    "        encoder.auxiliary_posterior.coding_sampler.auxiliary_vars = aux_vars\n",
    "    \n",
    "    if dont_run:\n",
    "        return encoder\n",
    "    else:\n",
    "        return encoder, *encoder.run_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pckled_stuff_var = pkl.load(open(f\"PickledStuff/BLR_RESULTS_v2/Dim{dim}/Variational_Epsilon{eps}_Beam{beamwidth}_Omega5.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_vars_var = pckled_stuff_var['aux_vars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c398764",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc, z, idx = encode_sample_var(t, aux_vars=aux_vars_var, epsilon=eps, beamwidth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a579ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traj(compute_variational_posterior(t), b, enc.selected_samples[0])\n",
    "plt.savefig(f\"Figures/Thesis/var_blr{dim}_eps{eps}_traj.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Figures/Thesis/var_blr{dim}_eps{eps}_traj.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cc4d1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0012, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         -0.0000],\n",
       "        [0.0000, 0.0007, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         -0.0000],\n",
       "        [0.0000, 0.0000, 0.0012, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         -0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         -0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0009, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         -0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0007, 0.0000, 0.0000, 0.0000,\n",
       "         -0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0006, 0.0000, 0.0000,\n",
       "         -0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0015, 0.0000,\n",
       "         -0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0010,\n",
       "         -0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0010]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_variational_posterior(t).covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2f8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
